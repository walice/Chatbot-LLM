{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26853e76",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chatbot development\n",
    "### Initial demo, 19 April 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50abd155",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Requirements\n",
    "We import `config` in order to access:\n",
    "- where the API key is stored\n",
    "- where the document directory is located\n",
    "- the prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046d229",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### API key\n",
    "- is not stored in plain text in `config.py`\n",
    "- is accessed as an environment variable (using an *.env* file that remains hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f9e0c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Prompt template\n",
    "The prompt is written so as to avoid \"hallucinations\"\n",
    "\n",
    "```python\n",
    "prompt_template = \"\"\"You are a personal Bot assistant for answering any questions about the documents.\n",
    "You are given a question and a set of documents.\n",
    "If the user's question requires you to provide specific information from the documents, give your answer based only on the examples provided below. DON'T generate an answer that is NOT written in the provided examples.\n",
    "If you don't find the answer to the user's question with the examples provided to you below, answer that you didn't find the answer in the documentation and propose him to rephrase his query with more details.\n",
    "Use bullet points if you have to make a list, only if necessary.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "DOCUMENTS:\n",
    "=========\n",
    "{context}\n",
    "=========\n",
    "Finish by proposing your help for anything else.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00dea2ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import OpenAI, VectorDBQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import config\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca33ea23",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "# Load documents from the specified directory using a DirectoryLoader object\n",
    "loader = DirectoryLoader(config.FILE_DIR, glob='*.pdf')\n",
    "documents = loader.load()\n",
    "\n",
    "# split the text to chuncks of of size 1000\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "# Split the documents into chunks of size 1000 using a CharacterTextSplitter object\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create a vector store from the chunks using an OpenAIEmbeddings object and a Chroma object\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=config.OPENAI_API_KEY)\n",
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936276fa",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Warning message: detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.**\n",
    "\n",
    "The warning message suggests that the Python package detectron2 is not installed on your system. detectron2 is an object detection library that provides various pre-trained models and tools to train custom object detection models.\n",
    "\n",
    "The warning message specifically mentions that the hi_res partitioning strategy is not available due to the absence of detectron2. The hi_res strategy is a partitioning strategy that is used to split large images into smaller tiles for efficient processing in object detection pipelines.\n",
    "\n",
    "Since detectron2 is not available, the object detection pipeline will fall back to the fast partitioning strategy. The fast strategy is a simpler approach that divides images into non-overlapping blocks of equal size. While the fast strategy is less computationally expensive than the hi_res strategy, it may not be suitable for processing large images or detecting small objects.\n",
    "\n",
    "If you require the hi_res partitioning strategy, you should install the detectron2 package using pip install detectron2. Once installed, the detectron2 package should be importable in your Python code, and you should be able to use the hi_res partitioning strategy in your object detection pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c37eb1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Using embedded DuckDB without persistence: data will be transient**\n",
    "\n",
    "The message you provided suggests that the current instance of DuckDB is running in an embedded mode without any persistence, which means that any data loaded into the database will not be saved to disk and will be transient.\n",
    "\n",
    "DuckDB is an embedded analytical SQL database engine that can be used for various data manipulation and querying tasks. When DuckDB is used in embedded mode, it runs inside the application process, and the database is not saved to disk by default.\n",
    "\n",
    "There are two modes of DuckDB: persistent mode and in-memory mode. In persistent mode, the database is saved to disk and can be used across multiple sessions. In-memory mode, on the other hand, does not save the database to disk, and any data loaded into the database will be lost once the application exits.\n",
    "\n",
    "The message you provided suggests that the current instance of DuckDB is running in in-memory mode without any persistence, which means that any data loaded into the database will be transient and will not be saved to disk.\n",
    "\n",
    "If you want to persist the data loaded into DuckDB, you should switch to persistent mode and specify a file path to save the database. This can be done by specifying the path to the database file as an argument to the duckdb.connect function, for example:\n",
    "\n",
    "```\n",
    "import duckdb\n",
    "con = duckdb.connect('my_database.db')\n",
    "```\n",
    "\n",
    "This will create a new database file named my_database.db in the current working directory and connect to it. Any data loaded into the database will be persisted in the file and can be accessed across multiple sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e0c2da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exploring object properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0da43536",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ce6b97",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "770"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e0545a2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Config', '__abstractmethods__', '__annotations__', '__class__', '__class_vars__', '__config__', '__custom_root_type__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__exclude_fields__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_validators__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__include_fields__', '__init__', '__init_subclass__', '__iter__', '__json_encoder__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__post_root_validators__', '__pre_root_validators__', '__pretty__', '__private_attributes__', '__reduce__', '__reduce_ex__', '__repr__', '__repr_args__', '__repr_name__', '__repr_str__', '__rich_repr__', '__schema_cache__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__try_update_forward_refs__', '__validators__', '__weakref__', '_abc_impl', '_calculate_keys', '_copy_and_set_values', '_decompose_class', '_embedding_func', '_enforce_dict_if_root', '_get_len_safe_embeddings', '_get_value', '_init_private_attributes', '_iter', 'chunk_size', 'client', 'construct', 'copy', 'dict', 'document_model_name', 'embed_documents', 'embed_query', 'embedding_ctx_length', 'from_orm', 'get_model_names', 'json', 'max_retries', 'model', 'openai_api_key', 'parse_file', 'parse_obj', 'parse_raw', 'query_model_name', 'schema', 'schema_json', 'update_forward_refs', 'validate', 'validate_environment']\n"
     ]
    }
   ],
   "source": [
    "print(dir(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bca4949",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_LANGCHAIN_DEFAULT_COLLECTION_NAME', '__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_client', '_client_settings', '_collection', '_embedding_function', '_persist_directory', 'add_documents', 'add_texts', 'as_retriever', 'delete_collection', 'from_documents', 'from_texts', 'max_marginal_relevance_search', 'max_marginal_relevance_search_by_vector', 'persist', 'similarity_search', 'similarity_search_by_vector', 'similarity_search_with_score']\n"
     ]
    }
   ],
   "source": [
    "print(dir(docsearch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4cd9e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Core of `chat.py` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da2f50eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function named 'answer' that takes a string prompt and an optional directory path\n",
    "# for persisting data. The function returns a string that represents the answer to the prompt.\n",
    "def answer(prompt: str, persist_directory: str = config.PERSIST_DIR) -> str:\n",
    "    \n",
    "    # Log a message indicating that the function has started\n",
    "    #LOGGER.info(f\"Start answering based on prompt: {prompt}.\")\n",
    "    \n",
    "    # Create a prompt template using a template from the config module and input variables\n",
    "    # representing the context and question.\n",
    "    prompt_template = PromptTemplate(template=config.prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    \n",
    "    # Load a QA chain using an OpenAI object, a chain type, and a prompt template.\n",
    "    doc_chain = load_qa_chain(\n",
    "        llm=OpenAI(\n",
    "            openai_api_key = config.OPENAI_API_KEY,\n",
    "            model_name=\"text-davinci-003\",\n",
    "            temperature=0,\n",
    "            max_tokens=300,\n",
    "        ),\n",
    "        chain_type=\"stuff\",\n",
    "        prompt=prompt_template,\n",
    "    )\n",
    "    \n",
    "    # Log a message indicating the number of chunks to be considered when answering the user's query.\n",
    "    #LOGGER.info(f\"The top {config.k} chunks are considered to answer the user's query.\")\n",
    "    \n",
    "    # Create a VectorDBQA object using a vector store, a QA chain, and a number of chunks to consider.\n",
    "    qa = VectorDBQA(vectorstore=docsearch, combine_documents_chain=doc_chain, k=config.k)\n",
    "    \n",
    "    # Call the VectorDBQA object to generate an answer to the prompt.\n",
    "    result = qa({\"query\": prompt})\n",
    "    answer = result[\"result\"]\n",
    "    \n",
    "    # Log a message indicating the answer that was generated\n",
    "    #LOGGER.info(f\"The returned answer is: {answer}\")\n",
    "    \n",
    "    # Log a message indicating that the function has finished and return the answer.\n",
    "    #LOGGER.info(f\"Answering module over.\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f212fa71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ee294da",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alice\\.conda\\envs\\chatbot\\lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:183: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nAnswer: Abonia Sojasingarayar is a Machine Learning Scientist, Data Scientist, NLP Engineer, Computer Vision Engineer, AI Analyst and Technical Writer. She has a Master's degree in Artificial Intelligence from IA School, a Master's degree in Digital Project Management from Institut F2I, and a Bachelor's degree in Computer Technology and Engineering from Université Pondicherry. She is Google certified professional data engineer, IBM Quantum Conversation Badge holder, and has Machine Learning and Natural Language Processing specialization from deeplearning.IA, Andrew NG. She is also proficient in French, English, Tamil and Hindi.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"Who is Abonia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd14347b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer: Illicit financial flows (IFFs) are international transfers of funds that were or are illegally obtained, transferred or used. They can also be any international transfers of wealth that are harmful to development. Examples of IFFs include trade misinvoicing, tariff evasion, corruption, and money laundering. I hope this answers your question. If you need any further help, please let me know.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"What are illicit financial flows?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ed970bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer: The carbon budget in 2050 is estimated to be 1198 GtC with a 33% chance of limiting warming to 2°C.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"What is the carbon budget in 2050?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d8f9027",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer: According to the documents provided, Ethiopia and Bangladesh are both classified as lower-middle income countries. The GDP growth rate for lower-middle income countries in 2030 and 2050 is 3.16% and 2.63%, respectively. Therefore, the total costs for Ethiopia and Bangladesh as a percentage of their GDP in 2030 and 2050 if they were to participate in the SkyShares reference scenario would be 3.16% and 2.63%, respectively. \\n\\nI hope this answers your question. Is there anything else I can help you with?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"What would be the total costs for Ethiopia and Bangladesh as a percentage of their GDP in 2030 and 2050 if they were to participate in the SkyShares reference scenario?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7858bfbd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2d32ccdf93beddb21d4aa6817d32bc13 in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2d32ccdf93beddb21d4aa6817d32bc13 in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2d32ccdf93beddb21d4aa6817d32bc13 in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 19 Apr 2023 14:23:31 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'agreed-earth-ltd', 'openai-processing-ms': '30733', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-reset-requests': '20ms', 'x-request-id': '2d32ccdf93beddb21d4aa6817d32bc13', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7ba5cd8c5d2f42c3-EWR', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe reference scenario in SkyShares is the \"Full Trade\" scenario, where the market-clearing price of allowances is determined endogenously by matching supply and demand for abatement. SkyShares then maps back the equilibrium price to each country’s marginal abatement cost (MAC) curve to determine how much abatement each country provides at the world price. The rest is traded.\\n\\nIs there anything else I can help you with?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"What is the reference scenario in SkyShares?\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
