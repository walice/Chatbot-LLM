{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26853e76",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chatbot development\n",
    "### Initial demo, 19 April 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50abd155",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Requirements\n",
    "We import `config` in order to access:\n",
    "- where the API key is stored\n",
    "- where the document directory is located\n",
    "- the prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c02520",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### API key\n",
    "- is not stored in plain text in `config.py`\n",
    "- is accessed as an environment variable (using an *.env* file that remains hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df7ca8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prompt template\n",
    "The prompt is written so as to avoid \"hallucinations\"\n",
    "\n",
    "```python\n",
    "prompt_template = \"\"\"You are a personal Bot assistant for answering any questions about the documents.\n",
    "You are given a question and a set of documents.\n",
    "If the user's question requires you to provide specific information from the documents, give your answer based only on the examples provided below. DON'T generate an answer that is NOT written in the provided examples.\n",
    "If you don't find the answer to the user's question with the examples provided to you below, answer that you didn't find the answer in the documentation and propose him to rephrase his query with more details.\n",
    "Use bullet points if you have to make a list, only if necessary.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "DOCUMENTS:\n",
    "=========\n",
    "{context}\n",
    "=========\n",
    "Finish by proposing your help for anything else.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00dea2ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import OpenAI, VectorDBQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import config\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca33ea23",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n",
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "# Load documents from the specified directory using a DirectoryLoader object\n",
    "loader = DirectoryLoader(config.FILE_DIR, glob='*.pdf')\n",
    "documents = loader.load()\n",
    "\n",
    "# split the text to chuncks of of size 1000\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "# Split the documents into chunks of size 1000 using a CharacterTextSplitter object\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create a vector store from the chunks using an OpenAIEmbeddings object and a Chroma object\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=config.OPENAI_API_KEY)\n",
    "docsearch = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936276fa",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Warning message: detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.**\n",
    "\n",
    "The warning message suggests that the Python package detectron2 is not installed on your system. detectron2 is an object detection library that provides various pre-trained models and tools to train custom object detection models.\n",
    "\n",
    "The warning message specifically mentions that the hi_res partitioning strategy is not available due to the absence of detectron2. The hi_res strategy is a partitioning strategy that is used to split large images into smaller tiles for efficient processing in object detection pipelines.\n",
    "\n",
    "Since detectron2 is not available, the object detection pipeline will fall back to the fast partitioning strategy. The fast strategy is a simpler approach that divides images into non-overlapping blocks of equal size. While the fast strategy is less computationally expensive than the hi_res strategy, it may not be suitable for processing large images or detecting small objects.\n",
    "\n",
    "If you require the hi_res partitioning strategy, you should install the detectron2 package using pip install detectron2. Once installed, the detectron2 package should be importable in your Python code, and you should be able to use the hi_res partitioning strategy in your object detection pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c37eb1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Using embedded DuckDB without persistence: data will be transient**\n",
    "\n",
    "The message you provided suggests that the current instance of DuckDB is running in an embedded mode without any persistence, which means that any data loaded into the database will not be saved to disk and will be transient.\n",
    "\n",
    "DuckDB is an embedded analytical SQL database engine that can be used for various data manipulation and querying tasks. When DuckDB is used in embedded mode, it runs inside the application process, and the database is not saved to disk by default.\n",
    "\n",
    "There are two modes of DuckDB: persistent mode and in-memory mode. In persistent mode, the database is saved to disk and can be used across multiple sessions. In-memory mode, on the other hand, does not save the database to disk, and any data loaded into the database will be lost once the application exits.\n",
    "\n",
    "The message you provided suggests that the current instance of DuckDB is running in in-memory mode without any persistence, which means that any data loaded into the database will be transient and will not be saved to disk.\n",
    "\n",
    "If you want to persist the data loaded into DuckDB, you should switch to persistent mode and specify a file path to save the database. This can be done by specifying the path to the database file as an argument to the duckdb.connect function, for example:\n",
    "\n",
    "```\n",
    "import duckdb\n",
    "con = duckdb.connect('my_database.db')\n",
    "```\n",
    "\n",
    "This will create a new database file named my_database.db in the current working directory and connect to it. Any data loaded into the database will be persisted in the file and can be accessed across multiple sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b503da9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exploring object properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0da43536",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ce6b97",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "840"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e0545a2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Config', '__abstractmethods__', '__annotations__', '__class__', '__class_vars__', '__config__', '__custom_root_type__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__exclude_fields__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_validators__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__include_fields__', '__init__', '__init_subclass__', '__iter__', '__json_encoder__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__post_root_validators__', '__pre_root_validators__', '__pretty__', '__private_attributes__', '__reduce__', '__reduce_ex__', '__repr__', '__repr_args__', '__repr_name__', '__repr_str__', '__rich_repr__', '__schema_cache__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__try_update_forward_refs__', '__validators__', '__weakref__', '_abc_impl', '_calculate_keys', '_copy_and_set_values', '_decompose_class', '_embedding_func', '_enforce_dict_if_root', '_get_len_safe_embeddings', '_get_value', '_init_private_attributes', '_iter', 'chunk_size', 'client', 'construct', 'copy', 'dict', 'document_model_name', 'embed_documents', 'embed_query', 'embedding_ctx_length', 'from_orm', 'get_model_names', 'json', 'max_retries', 'model', 'openai_api_key', 'parse_file', 'parse_obj', 'parse_raw', 'query_model_name', 'schema', 'schema_json', 'update_forward_refs', 'validate', 'validate_environment']\n"
     ]
    }
   ],
   "source": [
    "print(dir(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bca4949",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_LANGCHAIN_DEFAULT_COLLECTION_NAME', '__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_client', '_client_settings', '_collection', '_embedding_function', '_persist_directory', 'add_documents', 'add_texts', 'as_retriever', 'delete_collection', 'from_documents', 'from_texts', 'max_marginal_relevance_search', 'max_marginal_relevance_search_by_vector', 'persist', 'similarity_search', 'similarity_search_by_vector', 'similarity_search_with_score']\n"
     ]
    }
   ],
   "source": [
    "print(dir(docsearch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eb14ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Core of `chat.py` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da2f50eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function named 'answer' that takes a string prompt and an optional directory path\n",
    "# for persisting data. The function returns a string that represents the answer to the prompt.\n",
    "def answer(prompt: str, persist_directory: str = config.PERSIST_DIR) -> str:\n",
    "    \n",
    "    # Log a message indicating that the function has started\n",
    "    #LOGGER.info(f\"Start answering based on prompt: {prompt}.\")\n",
    "    \n",
    "    # Create a prompt template using a template from the config module and input variables\n",
    "    # representing the context and question.\n",
    "    prompt_template = PromptTemplate(template=config.prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    \n",
    "    # Load a QA chain using an OpenAI object, a chain type, and a prompt template.\n",
    "    doc_chain = load_qa_chain(\n",
    "        llm=OpenAI(\n",
    "            openai_api_key = config.OPENAI_API_KEY,\n",
    "            model_name=\"text-davinci-003\",\n",
    "            temperature=0,\n",
    "            max_tokens=300,\n",
    "        ),\n",
    "        chain_type=\"stuff\",\n",
    "        prompt=prompt_template,\n",
    "    )\n",
    "    \n",
    "    # Log a message indicating the number of chunks to be considered when answering the user's query.\n",
    "    #LOGGER.info(f\"The top {config.k} chunks are considered to answer the user's query.\")\n",
    "    \n",
    "    # Create a VectorDBQA object using a vector store, a QA chain, and a number of chunks to consider.\n",
    "    qa = VectorDBQA(vectorstore=docsearch, combine_documents_chain=doc_chain, k=config.k)\n",
    "    \n",
    "    # Call the VectorDBQA object to generate an answer to the prompt.\n",
    "    result = qa({\"query\": prompt})\n",
    "    answer = result[\"result\"]\n",
    "    \n",
    "    # Log a message indicating the answer that was generated\n",
    "    #LOGGER.info(f\"The returned answer is: {answer}\")\n",
    "    \n",
    "    # Log a message indicating that the function has finished and return the answer.\n",
    "    #LOGGER.info(f\"Answering module over.\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ca7d1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ee294da",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alice\\.conda\\envs\\chatbot\\lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:183: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`\n",
      "  warnings.warn(\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f3e2adf14022a5d5ed59ff27d9995b1b in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f3e2adf14022a5d5ed59ff27d9995b1b in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID f3e2adf14022a5d5ed59ff27d9995b1b in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 19 Apr 2023 15:04:00 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'agreed-earth-ltd', 'openai-processing-ms': '30009', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-reset-requests': '20ms', 'x-request-id': 'f3e2adf14022a5d5ed59ff27d9995b1b', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7ba608de7cb8c336-EWR', 'alt-svc': 'h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAnswer: Abonia Sojasingarayar is a Machine Learning Scientist, Data Scientist, NLP Engineer, Computer Vision Engineer, AI Analyst, and Technical Writer. She has a M2 in Artificial Intelligence from IA School in Boulogne-Billancourt, a M1 in Digital Project Management from Institut F2I in Paris, and a Licence in Computer Technology and Engineering from Université Pondicherry in Karikal, India. She is also certified in Google Professional Data Engineer, IBM Quantum Conversation Badge, Machine Learning Specialization by Deeplearning.IA, Andrew NG, Deep Learning & Natural Language Specialization by Deeplearning.IA, Andrew NG, Spark by Learning Academy, Advance Python Bootcamp, GCP Professional Data Engineer Badges, Watson Assistant Hands on, and Watson Foundation Methodology. She is also experienced in resolving data security and backup issues, disk cloning and configuration, and maintenance of PC post.\\n\\nI hope this answers your question. Is there anything else I can help you with?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"Who is Abonia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd14347b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer: Illicit financial flows (IFFs) are international transfers of funds that were or are illegally obtained, transferred or used. They can also be any international transfers of wealth that are harmful to development. Examples of IFFs include trade misinvoicing, tariff evasion, corruption, and money laundering. I hope this answers your question. If you need any further help, please let me know.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"What are illicit financial flows?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb7d2399",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer: The global carbon budget to stay below 2 degrees is 953 GtC, or 3,494 GtCO2. This budget is based on a 50% chance of limiting warming to 2°C. If mitigation begins immediately, emissions peak in 2019 at 40 billion metric tonnes of CO2, requiring emissions reduction rates of 6.2% a year from then on.\\n\\nI hope this answers your question. Is there anything else I can help you with?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"What is the global carbon budget to stay below 2 degrees?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a25c12d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer:\\nFor Ethiopia and Bangladesh, the total costs as a percentage of their GDP in 2030 and 2050 in the SkyShares reference scenario would be 27.23% and 10.96% respectively in 2030, and 0.56% and 2.97% respectively in 2050.\\n\\nIf you need help with anything else, please let me know.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"What would be the total costs for Ethiopia and Bangladesh as a percentage of their GDP in 2030 and 2050 if they were to participate in the SkyShares reference scenario?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc240d0c",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer: The reference scenario in SkyShares is a framework designed to limit global average warming to 2°C, with mitigation activity commencing immediately after agreement of the framework. All countries participate from the outset, and allocations of the global emissions budget converge from being in proportion to current emissions to start with, to equal per capita entitlements in the year 2030.\\n\\nI hope this answers your question. Is there anything else I can help you with?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(\"What is the reference scenario in SkyShares?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12921af6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## To discuss\n",
    "1. Validation\n",
    "    - What approach should be use to check that the chatbot is giving correct answers?\n",
    "        - Human experts\n",
    "        - Roll-out a mini experiment at AgreedEarth\n",
    "2. Fine-tuning\n",
    "3. Use cases to focus on\n",
    "4. Features to add\n",
    "    - See [Google Doc](https://docs.google.com/document/d/1pP2_LqOo7bLCFvV33FpfLxM_UlcMhrO2lmsaUyfqTcA/edit#)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
